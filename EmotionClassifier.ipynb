{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Training of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import clear_output\n",
    "pyplot.style.use(['dark_background'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fer2013/fer2013.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion labels\n",
    "df.emotion.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_text = {0: \"anger\",\n",
    "                    1: \"disgust\",\n",
    "                    2: \"fear\",\n",
    "                    3: \"happiness\",\n",
    "                    4: \"sadness\",\n",
    "                    5: \"surprise\",\n",
    "                    6: \"neutral\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string inputs of pixels to 48x48 grid for images\n",
    "first_img = df.pixels.loc[0]\n",
    "first_grid = np.array(first_img.split(\" \")).reshape(48, 48)\n",
    "first_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image using pyplot\n",
    "pyplot.imshow(first_grid.astype('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some sample data\n",
    "fig = pyplot.figure(1, (14, 14))\n",
    "k = 0\n",
    "for label in sorted(df.emotion.unique()):\n",
    "    for j in range(3):\n",
    "        px = df[df.emotion==label].pixels.iloc[k]\n",
    "        px = np.array(px.split(\" \")).reshape(48, 48).astype(\"float32\")\n",
    "        k += 1\n",
    "        ax = pyplot.subplot(7, 7, k)\n",
    "        ax.imshow(px)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(label_to_text[label])\n",
    "        pyplot.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply conversion function to all rows of data\n",
    "img_array = df.pixels.apply(lambda x : np.array(x.split(\" \")).reshape(48, 48, 1).astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange array\n",
    "img_array = np.stack(img_array, axis = 0)\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain array of emotion labels\n",
    "labels = df.emotion.values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(img_array, labels, test_size = 0.2)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data (images)\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sequential Model\n",
    "base_model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(filters=32, \n",
    "                                                               kernel_size=(3, 3), \n",
    "                                                               activation='relu', \n",
    "                                                               input_shape=(48,48,1)),\n",
    "                                        tf.keras.layers.MaxPool2D(2,2),\n",
    "                                        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "                                        tf.keras.layers.Conv2D(filters=64, \n",
    "                                                               kernel_size=(3, 3), \n",
    "                                                               activation='relu', \n",
    "                                                               input_shape=(48,48,1)),\n",
    "                                        tf.keras.layers.MaxPool2D(2,2),\n",
    "                                        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "                                        tf.keras.layers.Conv2D(filters=128, \n",
    "                                                               kernel_size=(3, 3), \n",
    "                                                               activation='relu', \n",
    "                                                               input_shape=(48,48,1)),\n",
    "                                        tf.keras.layers.MaxPool2D(2,2),\n",
    "                                        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "                                        tf.keras.layers.Flatten(),\n",
    "                                        tf.keras.layers.Dense(units=128, \n",
    "                                                              activation='relu'),\n",
    "                                        tf.keras.layers.Dropout(0.5),\n",
    "                                        tf.keras.layers.Dense(units=7, \n",
    "                                                              activation='softmax') # 7 emotion classes\n",
    "\n",
    "])\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                   loss='sparse_categorical_crossentropy', # Sparse as labels are integers (one-hot encoding)\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for storing model checkpoints\n",
    "try:\n",
    "    os.mkdir('checkpoints')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'best_model.keras'\n",
    "checkpoint_path = os.path.join('checkpoints', file_name) \n",
    "\n",
    "call_back = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                               monitor='val_accuracy',\n",
    "                                               verbose=1,\n",
    "                                               save_freq='epoch',\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=False,\n",
    "                                               mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start model training\n",
    "\n",
    "base_model.fit(x=x_train, \n",
    "               y=y_train,\n",
    "               epochs=20,\n",
    "               validation_split=0.2,\n",
    "               callbacks=call_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model performance against test data\n",
    "\n",
    "best_model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "for k in range(10):\n",
    "    print(f'actual label is {label_to_text[y_test[k]]}')\n",
    "    predicted_class = best_model.predict(tf.expand_dims(x_test[k], axis=0)).argmax()\n",
    "    print(f'predicted label is {label_to_text[predicted_class]}')\n",
    "    pyplot.imshow(x_test[k].reshape(48, 48))\n",
    "    pyplot.show()\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time prediction via webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up face classifier\n",
    "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Set up labels\n",
    "label_to_text = {0: \"anger\",\n",
    "                    1: \"disgust\",\n",
    "                    2: \"fear\",\n",
    "                    3: \"happiness\",\n",
    "                    4: \"sadness\",\n",
    "                    5: \"surprise\",\n",
    "                    6: \"neutral\"}\n",
    "\n",
    "# Obtain best model\n",
    "file_name = 'best_model.keras'\n",
    "checkpoint_path = os.path.join('checkpoints', file_name) \n",
    "best_model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "# Access webcam\n",
    "video_capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify emotions in video stream\n",
    "def detect_classify_emotion(vid):\n",
    "    # convert to grayscale for processing\n",
    "    img_gray = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces available in video stream\n",
    "    faces = face_classifier.detectMultiScale(img_gray, 1.1, 5, minSize = (40, 40))\n",
    "\n",
    "    # process each available face in video stream\n",
    "    for (x, y, w, h) in faces:\n",
    "        # draw bounding box around face\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "\n",
    "        # pre-process face for model prediction\n",
    "        face = img_gray[y:y + h, x:x + w]\n",
    "        cropped_face = np.expand_dims(np.expand_dims(cv2.resize(face, (48, 48)), -1), 0)\n",
    "\n",
    "        # preduct emotion using best model\n",
    "        predicted_emotion = best_model.predict(cropped_face)\n",
    "        max_index = int(np.argmax(predicted_emotion))\n",
    "        cv2.putText(vid, label_to_text[max_index], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    return faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use a loop for real-time face detection\n",
    "while True:\n",
    "\n",
    "    result, video_frame = video_capture.read() # read frames from video\n",
    "    if result is False:\n",
    "        break # terminate the loop if the frame is not read successfully\n",
    "\n",
    "    faces = detect_classify_emotion(video_frame)\n",
    "\n",
    "    cv2.imshow(\"Dylan's Emotion Classifier Project\", video_frame) # display processed frame in window\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_venv",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
